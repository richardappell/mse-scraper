{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraper for moneysavingexpert forum\n",
    "    Created 1st July 2021\n",
    "    Thanks for massive support to sydadder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to import\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_scrape(url, pages = 1, postdata = \"off\"):\n",
    "    \n",
    "    '''\n",
    "    ### url: Define url in string format to pull request from\n",
    "    ### pages: Define number of pages to search for threads on (default is first page only)\n",
    "    ### postdata: Sequences through all threads in a page and pulls the body of the thread post and date first created. \n",
    "    ### Default is turn postdata to off as it can be slow to to run.\n",
    "    '''\n",
    "    headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }\n",
    "\n",
    "    \n",
    "    ### Create dictionary for later creation of pandas DataFrame\n",
    "    if postdata == \"off\":\n",
    "        mse_questions = {\"rowhref\":[],\n",
    "                     \"rowtitle\":[],\n",
    "                     \"viewcount\":[],\n",
    "                     \"reply\":[]}\n",
    "        \n",
    "        page = []\n",
    "        for p in range(1, pages+1):\n",
    "            page.append(f\"{url}/p{p}\")\n",
    "        \n",
    "        for link in page:\n",
    "            urltoscrape = link\n",
    "    \n",
    "            reqs = req.get(urltoscrape, headers)\n",
    "            pool = soup(reqs.content, 'html.parser')\n",
    "            listofrows = pool.find_all(\"li\", class_=\"ItemDiscussion\")\n",
    "\n",
    "            for listrows in listofrows:\n",
    "                ### View Count\n",
    "                for s in listrows.find_all('span', class_=\"ViewCount\"):\n",
    "                    for num in s.find_all('span', class_=\"Number\"):\n",
    "                        viewcount = num.get_text()\n",
    "                ### Number of comments per post\n",
    "                for r in listrows.find_all('span', class_=\"CommentCount\"):\n",
    "                    for comm in r.find_all('span', class_=\"Number\"):\n",
    "                        reply = comm.get_text()\n",
    "                ### Get A Tag HREF AND TITLE\n",
    "                for a in listrows.find_all('a', href=True):\n",
    "                    if \"forums.moneysavingexpert.com\" in a['href']:\n",
    "                        rowhref = a['href']\n",
    "                        rowtitle = a.get_text()\n",
    "\n",
    "                    ### Storing data in dictionary\n",
    "                        mse_questions[\"rowhref\"].append(rowhref)\n",
    "                        mse_questions[\"rowtitle\"].append(rowtitle)\n",
    "                        mse_questions[\"viewcount\"].append(viewcount)\n",
    "                        mse_questions[\"reply\"].append(reply)\n",
    "    \n",
    "        df_mse = pd.DataFrame(mse_questions)\n",
    "        df_mse.to_csv(\"mse_questions.csv\")\n",
    "        print(\"CSV file saved in your local directory\")\n",
    "    \n",
    "    \n",
    "    ## Create dictionary for later creation of pandas DataFrame - with added body and date of irig\n",
    "    if postdata == \"on\":\n",
    "        mse_questions = {\"rowhref\":[],\n",
    "                         \"rowtitle\":[],\n",
    "                         \"viewcount\":[],\n",
    "                         \"reply\":[],\n",
    "                        \"body\":[],\n",
    "                        \"date_created\":[]}\n",
    "        \n",
    "        page = []\n",
    "        for p in range(1, pages+1):\n",
    "            page.append(f\"{url}/p{p}\")\n",
    "            \n",
    "        for link in page:\n",
    "            urltoscrape = link\n",
    "\n",
    "            reqs = req.get(urltoscrape, headers)\n",
    "            pool = soup(reqs.content, 'html.parser')\n",
    "            listofrows = pool.find_all(\"li\", class_=\"ItemDiscussion\")\n",
    "\n",
    "            for listrows in listofrows:\n",
    "                ### View Count\n",
    "                for s in listrows.find_all('span', class_=\"ViewCount\"):\n",
    "                    for num in s.find_all('span', class_=\"Number\"):\n",
    "                        viewcount = num.get_text()\n",
    "                ### Number of comments per post\n",
    "                for r in listrows.find_all('span', class_=\"CommentCount\"):\n",
    "                    for comm in r.find_all('span', class_=\"Number\"):\n",
    "                        reply = comm.get_text()\n",
    "                ### Get A Tag HREF AND TITLE\n",
    "                for a in listrows.find_all('a', href=True):\n",
    "                    if \"forums.moneysavingexpert.com\" in a['href']:\n",
    "                        rowhref = a['href']\n",
    "                        rowtitle = a.get_text()\n",
    "\n",
    "                        url = rowhref\n",
    "                \n",
    "                    reqs = req.get(url)\n",
    "                    nextlevel = soup(reqs.content, 'html.parser')\n",
    "                    body = nextlevel.find(\"div\", class_=\"Message userContent\")\n",
    "\n",
    "                if body == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    result_body = body.get_text().strip()\n",
    "\n",
    "\n",
    "                date_created = nextlevel.find(\"span\", class_=\"MItem DateCreated\").get_text().strip()\n",
    "\n",
    "                if \"Today\" in date_created:\n",
    "                    date_created = date_created.replace(\"Today\", dt.datetime.now().strftime(\"%d %B\"))\n",
    "                    date_parsed = parse(date_created)\n",
    "\n",
    "                else:\n",
    "                    date_parsed = parse(date_created)\n",
    "\n",
    "              ### Storing data in dictionary for later\n",
    "                mse_questions[\"rowhref\"].append(rowhref)\n",
    "                mse_questions[\"rowtitle\"].append(rowtitle)\n",
    "                mse_questions[\"viewcount\"].append(viewcount)\n",
    "                mse_questions[\"reply\"].append(reply)\n",
    "                mse_questions[\"body\"].append(result_body)\n",
    "                mse_questions[\"date_created\"].append(date_parsed)\n",
    "\n",
    "        df_mse = pd.DataFrame(mse_questions)\n",
    "        df_mse.to_csv(\"mse_questionsdetail.csv\")\n",
    "        print(\"CSV file saved in your local directory\")\n",
    "        \n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
